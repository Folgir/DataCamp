
#	1. Clustering for dataset exploration
BEGIN
	Unsupervised learning:
		# Unsupervised learning finds patterns in data
		# E.g. clustering customers by their purchases
		# Compressing the data using purchase patterns(dimension reduction)
	
	k-means clustering with scikit-learn
		# Import KMeans
		from sklearn.cluster import KMeans
		# Create a KMeans instance with 3 clusters: model
		model = KMeans(n_clusters=3)
		# Fit model to points
		model.fit(points)
		# Determine the cluster labels of new_points: labels
		labels = model.predict(new_points)
		# Print cluster labels of new_points
		print(labels)

	To visualize clusters
		# Import pyplot
		import matplotlib.pyplot as plt
		# Assign the columns of new_points: xs and ys
		xs = new_points[:,0]
		ys = new_points[:,1]
		# Make a scatter plot of xs and ys, using labels to define the colors
		plt.scatter(xs, ys, c=labels, alpha=0.5)
		# Assign the cluster centers: centroids
		centroids = model.cluster_centers_
		# Assign the columns of centroids: centroids_x, centroids_y
		centroids_x = centroids[:,0] # 3 Cluster centers, x axis coordinates
		centroids_y = centroids[:,1] # 3 Cluster centers, y axis coordinates
		# Make a scatter plot of centroids_x and centroids_y
		plt.scatter(centroids_x, centroids_y, marker='D', s=50) # D-diamond shape, s-size
		plt.show()
		
	Measuring clustering quality
		# A good clustering has tight clusters;
		# Inertia measures how spread out the clusters are(lower is better)
		# Inertia - Distance from each sample to centroid cluster
		# In sklearn after fit(), inertia is available as attribute inertia_
		# To choose good k parameter draw inertia/k chart;
			# Choose point where inertia begins to decrease more slowly (elbow of chart)
		
		# Draw chart to choose good k parameter value!
			ks = range(1, 6)
			inertias = []
			for k in ks:
				# Create a KMeans instance with k clusters: model
				model = KMeans(n_clusters=k)
				# Fit model to samples
				model.fit(samples)
				# Append the inertia to the list of inertias
				inertias.append(model.inertia_)
				
			# Plot ks vs inertias
			plt.plot(ks, inertias, '-o')
			plt.xlabel('number of clusters, k')
			plt.ylabel('inertia')
			plt.xticks(ks)
		
		
		Use Cross-tabulation to evaluate clustering results
			import pandas as pd
			from sklearn.cluster import KMeans
			
			# Create a KMeans model with 3 clusters: model
			model = KMeans(n_clusters=3)
			# Use fit_predict to fit model and obtain cluster labels: labels
			labels = model.fit_predict(samples) # fit_predict = .fit() and then .predict()
			# Create a DataFrame with labels and varieties as columns: df
			df = pd.DataFrame({'labels': labels, 'varieties': varieties})
			# Create crosstab: ct
			ct = pd.crosstab(df['labels'], df['varieties'])
		
		StandardScaler - transforms each feature to have mean 0 and variance 1
			# StandardScaler use fit() / transform()
			# TO GET BETTER RESULTS: 
				# Try without rescaling, with standardized dataset and with normalized dataset and compare results
				
		Pipeline example
			# Perform the necessary imports
			from sklearn.cluster import KMeans
			from sklearn.preprocessing import StandardScaler
			
			import pandas as pd
			
			# Create scaler: scaler
			scaler = StandardScaler()
			# Create KMeans instance: k-means
			kmeans = KMeans(n_clusters=4)
			# Create pipeline: pipeline
			pipeline = make_pipeline(scaler, kmeans)

			# Fit the pipeline to samples
			pipeline.fit(samples)
			# Calculate the cluster labels: labels
			labels = pipeline.predict(samples)
			# Create a DataFrame with labels and species as columns: df
			df = pd.DataFrame({'labels': labels, 'species': species})
			# Create crosstab: ct
			ct = pd.crosstab(df['labels'], df['species'])
			
		Normalizer Example
			# Import Normalizer
			from sklearn.preprocessing import Normalizer

			# Create a normalizer: normalizer
			normalizer = Normalizer()
				
				# For cases where cannot be used pipeline, normalize instead of Normalizer could be used
				# Import normalize
				from sklearn.preprocessing import normalize

				# Normalize the movements: normalized_movements
				normalized_movements = normalize()


		
	End

	
	
#	2. Visualization with hierarchical clustering and t-SNE
BEGIN

	Hierarchical clustering
		#	Aglomerative Hierarchical Clustering = Bottom Up
	
		# Perform the necessary imports
		from scipy.cluster.hierarchy import linkage, dendrogram		#	Hierarchical clustering is in scipy not sklearn!!!!
		import matplotlib.pyplot as plt

		# Calculate the linkage: mergings
		mergings = linkage(samples, method='complete') 	#  SciPy linkage() function performs hierarchical clustering on an array of samples. 
		# Plot the dendrogram, using varieties as labels
		dendrogram(mergings, labels=varieties, leaf_rotation=90, leaf_font_size=6)	#	use dendrogram() to visualize the result
		plt.show()	   
				   
		# Import normalize
		from sklearn.preprocessing import normalize
		
		# Normalize the movements: normalized_movements
		normalized_movements = normalize(movements)
		# Calculate the linkage: mergings
		mergings = linkage(normalized_movements, method='complete')
		# Plot the dendrogram
		dendrogram(mergings, labels=companies, leaf_rotation=90, leaf_font_size=6)
		plt.show()
	
	Visualizing intermediate stages of hierarchical clustering
		# Intermediate clusterings are choosen by taking height number from dendogram and gathering all bellow in 1 cluster
		# Height on dendrogram is distance between merging clusters
		# Distance between 2 clusters is measured using linkage method!!!!
			# linkage(x, method='complete') - distance between clusters  is max. distance between their samples
				# In complete linkage, the distance between clusters is the distance between the furthest points of the clusters.
				# In single linkage, the distance between clusters is the distance between the closest points of the clusters.
				mergings = linkage(samples, method='single')
		# To extract intermediate cluster labels need to use fcluster method!!!!
		
		# Perform the necessary imports
		import pandas as pd
		from scipy.cluster.hierarchy import fcluster

		# Use fcluster to extract labels: labels
		labels = fcluster(mergings, t=6, criterion='distance')		#	mergings = linkage(x, method='single')
		# Create a DataFrame with labels and varieties as columns: df
		df = pd.DataFrame({'labels': labels, 'varieties': varieties})
		# Create crosstab: ct
		ct = pd.crosstab(df['labels'], df['varieties'])
		# Display ct
		print(ct)	
	
	t-SNE for 2-dimensional maps
		#	Maps many dimensional samples to 2D or 3D space
		#	Map approximately preserves nearness of samples
		# 	Great for inspecting samples
		#	TSNE - have only fit_transform() method (no fit() or transform() separately) as result you cannot fit and then get on with new samples. Need to rerun each time if new data added!!!!
		# 	TSNE learning rate - wrong choice: points bunch together (try values between 50 and 200)
		#	TSNE axis dont have any interpretable meaning, they are different each time !!!!!
		
		# Import TSNE
		from sklearn.manifold import TSNE

		# Create a TSNE instance: model
		model = TSNE(learning_rate=200)
		# Apply fit_transform to samples: tsne_features
		tsne_features = model.fit_transform(samples)
		# Select the 0th feature: xs
		xs = tsne_features[:,0]
		# Select the 1st feature: ys
		ys = tsne_features[:,1]
		# Scatter plot, coloring by variety_numbers
		plt.scatter(xs, ys, c=variety_numbers)
		plt.show()	
	
		# Import TSNE
		from sklearn.manifold import TSNE

		# Create a TSNE instance: model
		model = TSNE(learning_rate=50)
		# Apply fit_transform to normalized_movements: tsne_features
		tsne_features = model.fit_transform(normalized_movements)
		# Select the 0th feature: xs
		xs = tsne_features[:,0]
		# Select the 1th feature: ys
		ys = tsne_features[:,1]
		# Scatter plot
		plt.scatter(xs, ys, alpha=0.5)
		# Annotate the points
		for x, y, company in zip(xs, ys, companies):
			plt.annotate(company, (x, y), fontsize=5, alpha=0.75)
		plt.show()
	end
	
	
	
#	3. Decorrelating your data and dimension reduction
BEGIN
	#	PCA - Principal Component Analysis
		#	Step 1 - decorrelation
			#	Rotates data samples to be aligned with axes (as result looses linear correlation)
			#	Shifts data samples so they have mean 0
			#	No information lost
			#	PCA - Principal Components - directions of variances
			#	PCA - should align with the axis of the point cloud, basically if points are moving up to the left then 1 component is up to the left and other is 90 degree from that line!!!
			#	PCA - 	fit() - learns the transformation for given data
			#			transform() - applies the learned transformation, can also be applied to new data!!!
			
				# Perform the necessary imports
				import matplotlib.pyplot as plt
				from scipy.stats import pearsonr

				# Assign the 0th column of grains: width
				width = grains[:,0]
				# Assign the 1st column of grains: length
				length = grains[:,1]
				# Scatter plot width vs length
				plt.scatter(width, length)
				plt.axis('equal')
				plt.show()
				# Calculate the Pearson correlation
				correlation, pvalue = pearsonr(width, length)
				# Display the correlation
				print(correlation)		#	r==0.8

				# Import PCA
				from sklearn.decomposition import PCA

				# Create PCA instance: model
				model = PCA()
				# Apply the fit_transform method of model to grains: pca_features
				pca_features = model.fit_transform(grains)
				# Assign 0th column of pca_features: xs
				xs = pca_features[:,0]
				# Assign 1st column of pca_features: ys
				ys = pca_features[:,1]
				# Scatter plot xs vs ys
				plt.scatter(xs, ys)
				plt.axis('equal')
				plt.show()
				# Calculate the Pearson correlation of xs and ys
				correlation, pvalue = pearsonr(xs, ys)
				# Display the correlation
				print(correlation)		#	r==0 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

		#	Step 2 - find intrinsic dimension ammount/count
			#	Intrinsic dimension = number of features needed to aproximate the dataset
			#	PCA identifies intrinsic dimension when sample have any number of features
			#	Intrinsic dimension = number of PCA features with significant variances (PCA features is calculated at decorrelation phase and show directions)
			#	PCA features are ordered by variance descending
			#	The first principal component of the data is the direction in which the data varies the most.

			# Make a scatter plot of the untransformed points
			plt.scatter(grains[:,0], grains[:,1])
			# Create a PCA instance: model
			model = PCA()
			# Fit model to points
			model.fit(grains)
			# Get the mean of the grain samples: mean
			mean = model.mean_
			# Get the first principal component: first_pc 		The first principal component of the data is the direction in which the data varies the most!!!!
			first_pc = model.components_[0,:]
			# Plot first_pc as an arrow, starting at mean
			plt.arrow(mean[0], mean[1], first_pc[0], first_pc[1], color='red', width=0.01)
			# Keep axes on same scale
			plt.axis('equal')
			plt.show()

			# Perform the necessary imports
			from sklearn.decomposition import PCA
			from sklearn.preprocessing import StandardScaler
			from sklearn.pipeline import make_pipeline
			import matplotlib.pyplot as plt

			# Create scaler: scaler
			scaler = StandardScaler()
			# Create a PCA instance: pca
			pca = PCA()
			# Create pipeline: pipeline
			pipeline = make_pipeline(scaler, pca)
			# Fit the pipeline to 'samples'
			pipeline.fit(samples)

			# Plot the explained variances
			features = range(pca.n_components_)			#	n_components = feature count of data ?(columns)
			plt.bar(features, pca.explained_variance_)	#	explained_variance_ is thing what need to be maximized, the higher the better, means more intrinsic features there are
			plt.xlabel('PCA feature')
			plt.ylabel('variance')
			plt.xticks(features)
			plt.show()
			
		#	Step 3 - reduce dimensions
			#	Dimension reduction - represent same data, using less features
			#	PCA variance results - assumes low variance features - 'noise' and high variance features to be informative
			#	PCA(n_components=2)		this is where you set how many dimensions to keep, good choice is to keep intrinsic dimension ammount (n_components and n_components_ are not same)
			
			# Import PCA
			from sklearn.decomposition import PCA

			# Create a PCA model with 2 components: pca
			pca = PCA(n_components=2)
			# Fit the PCA instance to the scaled samples
			pca.fit(scaled_samples)
			# Transform the scaled samples: pca_features
			pca_features = pca.transform(scaled_samples)
			# Print the shape of pca_features
			print(pca_features.shape)
		
		#	Work with sparse matrices
		
			#	Array is sparse = most entries are zero
			#	To save space of memory scipy.sparse.csr_matrix could be used (it remembers only places were no 0 are present)
			#	scikit-learn PCA doesnot support csr_matrix, and need to use TruncatedSVD instead (does same as PCA but accepts sparse matrices as input
			
			#	tf-idf:
				#	tf = frequency of word count in document	0.1 means that in document 10% of time was used this word!	
				#	idf -> reduces influence of frequent words (the, a, e.t.c.)
						
			#	you'll create a tf-idf word frequency array for a toy collection of documents
			#	TfidfVectorizer transforms a list of documents into a word frequency array, which it outputs as a csr_matrix.
			
			#	Create sparse matrix of word count - word frequency array
			# Import TfidfVectorizer
			from sklearn.feature_extraction.text import TfidfVectorizer
			# Create a TfidfVectorizer: tfidf
			tfidf = TfidfVectorizer() 
			# Apply fit_transform to document: csr_mat
			csr_mat = tfidf.fit_transform(documents)		#	documents - list
			# Print result of toarray() method
			print(csr_mat.toarray())
			# Get the words: words
			words = tfidf.get_feature_names()
			# Print words
			print(words)

			#	Seting up pipeline and Dimension reduction
			# Perform the necessary imports
			from sklearn.decomposition import TruncatedSVD
			from sklearn.cluster import KMeans
			from sklearn.pipeline import make_pipeline
			# Create a TruncatedSVD instance: svd
			svd = TruncatedSVD(n_components=50)		#	TruncatedSVD is PCA for Sparse matrices!!!!
			# Create a KMeans instance: kmeans
			kmeans = KMeans(n_clusters=6)
			# Create a pipeline: pipeline
			pipeline = make_pipeline(svd, kmeans)

			#	Clustering
			# Import pandas
			import pandas as pd
			# Fit the pipeline to articles
			pipeline.fit(articles)
			# Calculate the cluster labels: labels
			labels = pipeline.predict(articles)
			# Create a DataFrame aligning labels and titles: df
			df = pd.DataFrame({'label': labels, 'article': titles})
			# Display df sorted by cluster label
			print(df.sort_values(by='label'))
	end
	
	
	
#	4. Discovering interpretable features
BEGIN
	#	NMF - Non-negative matrix factorization
		#	Dimension reduction technique
		#	NMF models are interpretable (unlike PCA)
		#	Easy to interpret-explain
		#	NMF could be applied only to positive sample features(>=0), CANNOT BE APPLIED TO NEGATIVE FEATURES!!!!
		
		#	NMF have fit() and transform()
		#	Always must specify number of components NMF(n_components=2)
		#	Works with NumPy arrays and with csr_matrix(sparse matrices)
		
		#	NMF has components(just like PCA)
		#	Dimension of components = dimension of samples
		
		#	How it works:
			#	sample values[i, :] 	= 	[0.12, 0.18, 0.32, 0.14]
			#	nmf_features[i, :] 		= 	[0.15, 0.12]							#	Calculated from model
			#	model components in [] 		 	0.15 	* 	[[0.01 	0. 		2.13 	0.54]
			#					 				+0.12 	*  	[0.99 	1.47 	0. 		0.5]]
			#					 				= 		  	[0.1203	0.1764	0.3195	0.141]

		# Word frequency vectors
			# Import NMF
			from sklearn.decomposition import NMF
			# Create an NMF instance: model
			model = NMF(n_components=6)
			# Fit the model to articles
			model.fit(articles)			#	articles <- csr matrix
			# Transform the articles: nmf_features
			nmf_features = model.transform(articles)
			# Print the NMF features
			print(nmf_features)			
				
			# Import pandas
			import pandas as pd
			# Create a pandas DataFrame: df
			df = pd.DataFrame(nmf_features, index=titles)	#	nmf_features = from previous step - NumPy; titles <- list
			# Print the row for 'Anne Hathaway'
			print(df.loc['Anne Hathaway'])		#	->	3rd feature have the highest number!!!!

			# Import pandas
			import pandas as pd
			# Create a DataFrame: components_df
			components_df = pd.DataFrame(model.components_, columns=words)
			# Print the shape of the DataFrame
			print(components_df.shape)
			# Select row 3: component
			component = components_df.iloc[3,:]
			# Print result of nlargest
			print(component.nlargest())		#	5 largest values


		#	Images
			# Grayscale image - no colors, only shades of gray, measure pixel brightness with values from 0 (black) to 1 (white)
			# Image is matrix of those brightness values, then it can be flatten - converted to 1D array, Row by row from left to right
			# Then to encode a collection of images: images same size are encoded in 2D array, where Row is each image, and column represents pixel
		
		#	Create 2D array from 1D and then plot it to get original image back
			# Import pyplot
			from matplotlib import pyplot as plt
			# Select the 0th row: digit
			digit = samples[0,:]
			# Print digit
			print(digit)
			# Reshape digit to a 13x8 array: bitmap
			bitmap = digit.reshape(13, 8)
			# Print bitmap
			print(bitmap)
			# Use plt.imshow to display bitmap
			plt.imshow(bitmap, cmap='gray', interpolation='nearest')
			plt.colorbar()
			plt.show()

		#	Show components of NMF model - NMF has expressed the digit as a sum of the components!!!
			def show_as_image(sample):
				bitmap = sample.reshape((13, 8))
				plt.figure()
				plt.imshow(bitmap, cmap='gray', interpolation='nearest')
				plt.colorbar()
				plt.show()
	
			# Import NMF
			from sklearn.decomposition import NMF
			# Create an NMF model: model
			model = NMF(n_components=7)
			# Apply fit_transform to samples: features
			features = model.fit_transform(samples)
			# Call show_as_image on each component
			for component in model.components_:
				show_as_image(component)
			# Assign the 0th row of features: digit_features
			digit_features = features[0,:]
			# Print digit_features
			print(digit_features)
		
		#	Same example with PCA - show that PCA is not meaningful tool to decompose images in components
			# Import PCA
			from sklearn.decomposition import PCA
			# Create a PCA instance: model
			model = PCA(n_components=7)
			# Apply fit_transform to samples: features
			features = model.fit_transform(samples)
			# Call show_as_image on each component
			for component in model.components_:
				show_as_image(component)			#	show_as_image - created function to plot images


				
		# 	Building recommender systems using NMF
			# NMF applied to word-frequency arrays
			# NMF feature values describe the topics, so similar articles will have similar NMF feature values
			# COMPARE NMF feature values:
				# compare using Cosine similarity, this compares angles between lines(created from features), higher values means more similar (maximum is 1 when angle is 0)
		
			Perform the necessary imports
			import pandas as pd
			from sklearn.preprocessing import normalize
			# Normalize the NMF features: norm_features
			norm_features = normalize(nmf_features)
			# Create a DataFrame: df
			df = pd.DataFrame(norm_features, index=titles)
			# Select the row corresponding to 'Cristiano Ronaldo': article
			article = df.loc['Cristiano Ronaldo']
			# Compute the dot products: similarities
			similarities = df.dot(article)				#	Apply the .dot() method of df to article to calculate the cosine similarity of every row with article
			
		#	Build music recommender		
			# Perform the necessary imports
			from sklearn.decomposition import NMF
			from sklearn.preprocessing import Normalizer, MaxAbsScaler
			from sklearn.pipeline import make_pipeline
			# Create a MaxAbsScaler: scaler
			scaler = MaxAbsScaler()
			# Create an NMF model: nmf
			nmf = NMF(n_components=20)
			# Create a Normalizer: normalizer
			normalizer = Normalizer()
			# Create a pipeline: pipeline
			pipeline = make_pipeline(scaler, nmf, normalizer)
			# Apply fit_transform to artists: norm_features
			norm_features = pipeline.fit_transform(artists)	
			
			# Import pandas
			import pandas as pd
			# Create a DataFrame: df
			df = pd.DataFrame(norm_features, index=artist_names)
			# Select row of 'Bruce Springsteen': artist
			artist = df.loc['Bruce Springsteen']
			# Compute cosine similarities: similarities
			similarities = df.dot(artist)
			# Display those with highest cosine similarity
			print(similarities.nlargest())
	end